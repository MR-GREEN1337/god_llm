{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to God LLM Documentation","text":"<p>God LLM is a thought exploration framework that helps you model and analyze complex systems of beliefs and ideas.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Flexible node-based system for representing thoughts and concepts</li> <li>Plugin system for different LLM providers</li> <li>Customizable scoring metrics</li> <li>Rich template system</li> <li>Extensive utilities for analysis</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Core Concepts</li> <li>API Reference</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to the God Module! This guide will help you get started with development contributions.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/mr-green1337/god_llm.git\ncd god_llm\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Unix\n# or\n.\\venv\\Scripts\\activate  # Windows\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-install-pre-commit-hooks","title":"3. Install Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#code-standards","title":"Code Standards","text":""},{"location":"contributing/#style-guide","title":"Style Guide","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use type hints for all functions</li> <li>Maximum line length: 88 characters (Black formatter)</li> <li>Docstring format: Google style</li> </ul> <pre><code>def example_function(param1: str, param2: int) -&gt; bool:\n    \"\"\"Does something useful with the parameters.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: Description of when this error occurs\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write unit tests for all new features</li> <li>Maintain minimum 90% test coverage</li> <li>Place tests in <code>tests/</code> directory</li> </ul> <pre><code># tests/test_example.py\nimport pytest\nfrom god_llm.core import God\n\ndef test_expansion():\n    god = God(llm=MockLLM())\n    result = god.expand(\"test prompt\")\n    assert isinstance(result, str)\n    assert len(result) &gt; 0\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Fork and Branch <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-fix-name\n</code></pre></p> </li> <li> <p>Commit Messages <pre><code># Format:\n# type(scope): description\n\ngit commit -m \"feat(core): add new expansion algorithm\"\ngit commit -m \"fix(templates): correct prompt formatting\"\n</code></pre></p> </li> <li> <p>Testing <pre><code># Run tests\npytest\n\n# Check coverage\npytest --cov=god_llm tests/\n</code></pre></p> </li> <li> <p>Documentation</p> </li> <li>Update relevant documentation</li> <li>Add docstrings for new functions</li> <li>Include example usage</li> </ol>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<ol> <li> <p>New LLM Provider <pre><code>from god_llm.llm import BaseLLM\n\nclass NewLLMProvider(BaseLLM):\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def generate(self, prompt: str) -&gt; str:\n        # Implementation\n        pass\n\n    def get_embedding(self, text: str) -&gt; List[float]:\n        # Implementation\n        pass\n</code></pre></p> </li> <li> <p>New Template <pre><code>from god_llm.templates import TemplateManager\n\ndef add_custom_template(template_manager: TemplateManager):\n    template_manager.add_template(\n        \"new_template\",\n        \"Custom prompt format: {variable}\"\n    )\n</code></pre></p> </li> <li> <p>New Scoring Method <pre><code>from god_llm.scoring import BaseScorer\n\nclass CustomScorer(BaseScorer):\n    def compute_score(self, text: str) -&gt; float:\n        # Implementation\n        pass\n</code></pre></p> </li> </ol>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<pre><code>from god_llm.exceptions import GodError\n\nclass CustomError(GodError):\n    \"\"\"Custom error for specific cases.\"\"\"\n    pass\n\ndef example_function():\n    try:\n        # Implementation\n        pass\n    except Exception as e:\n        raise CustomError(f\"Meaningful error message: {str(e)}\")\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":"<ol> <li> <p>Version Bump <pre><code># Update version in setup.py\n# Update CHANGELOG.md\ngit commit -m \"chore: bump version to X.Y.Z\"\n</code></pre></p> </li> <li> <p>Create Release <pre><code>git tag vX.Y.Z\ngit push origin vX.Y.Z\n</code></pre></p> </li> <li> <p>Build and Upload <pre><code>python setup.py sdist bdist_wheel\ntwine upload dist/*\n</code></pre></p> </li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue for bugs</li> <li>Use discussions for questions</li> <li>Join our Discord community</li> </ul>"},{"location":"api/core/","title":"Core API Reference","text":"<p>Write content for Core API Reference here.</p>"},{"location":"api/core/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"api/core/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"api/plugins/","title":"Plugins API Reference","text":"<p>Write content for Plugins API Reference here.</p>"},{"location":"api/plugins/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"api/plugins/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"api/templates/","title":"Templates API Reference","text":"<p>Write content for Templates API Reference here.</p>"},{"location":"api/templates/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"api/templates/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"api/utils/","title":"Utils API Reference","text":"<p>Write content for Utils API Reference here.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"api/utils/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"concepts/god/","title":"God Module Documentation","text":""},{"location":"concepts/god/#overview","title":"Overview","text":"<p>The God module represents the core orchestrator of the contextual reasoning system. Named with inspiration from Proverbs 2:6, it manages the generation, evaluation, and exploration of knowledge paths using Large Language Models.</p>"},{"location":"concepts/god/#class-god","title":"Class: God","text":""},{"location":"concepts/god/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>def __init__(\n    llm: BaseLLM,\n    similarity_threshold: float = 0.7,\n    max_iterations: int = 4,\n    context_window: int = 3,\n    min_question_score: MinQuestionScore = MinQuestionScore(min_question_score=0.4),\n    template_manager: Optional[TemplateManager] = None,\n    debug: bool = False\n)\n</code></pre> <ul> <li><code>llm</code>: Base language model interface</li> <li><code>similarity_threshold</code>: Threshold for node relationships (0.0-1.0)</li> <li><code>max_iterations</code>: Maximum depth of reasoning chains</li> <li><code>context_window</code>: Number of previous nodes to maintain in context</li> <li><code>min_question_score</code>: Minimum score for generated questions</li> <li><code>template_manager</code>: Manager for prompt templates</li> <li><code>debug</code>: Enable detailed logging</li> </ul>"},{"location":"concepts/god/#core-methods","title":"Core Methods","text":""},{"location":"concepts/god/#expand","title":"expand()","text":"<pre><code>def expand(\n    prompt: str, \n    parent_id: Optional[str] = None, \n    depth: int = 0, \n    retry_count: int = 0, \n    max_nodes: int = 15\n) -&gt; str\n</code></pre> <p>Expands a prompt into a thought and generates follow-up questions: - Creates new node with response to prompt - Generates and filters follow-up questions - Builds relationships with existing nodes - Implements retry logic for irrelevant responses - Returns node ID of created node</p>"},{"location":"concepts/god/#pray","title":"pray()","text":"<pre><code>def pray(k: int = 3) -&gt; Dict[str, List[Dict]]\n</code></pre> <p>Discovers and evaluates knowledge paths: - Finds all possible paths through nodes - Computes comprehensive path scores - Returns top-k paths for each root node - Includes detailed metrics and trajectory information</p>"},{"location":"concepts/god/#miracle","title":"miracle()","text":"<pre><code>def miracle(k: int = 3) -&gt; Dict[str, List[Dict]]\n</code></pre> <p>Generates human-readable summaries of best paths: - Builds on pray() operation - Creates natural language summaries - Includes scores and metrics - Returns structured results dictionary</p>"},{"location":"concepts/god/#internal-methods","title":"Internal Methods","text":""},{"location":"concepts/god/#context-management","title":"Context Management","text":"<ul> <li><code>_build_context_history()</code>: Builds context from previous nodes</li> <li><code>_create_enhanced_prompt()</code>: Enhances prompts with context</li> <li><code>_find_relations()</code>: Discovers node relationships</li> </ul>"},{"location":"concepts/god/#quality-control","title":"Quality Control","text":"<ul> <li><code>_is_node_relevant()</code>: Evaluates node relevance</li> <li><code>_generate_and_filter_questions()</code>: Creates and filters questions</li> <li><code>_delete_node()</code>: Removes irrelevant nodes</li> </ul>"},{"location":"concepts/god/#path-analysis","title":"Path Analysis","text":"<ul> <li><code>_find_paths()</code>: Discovers all possible paths</li> <li><code>_compute_path_score()</code>: Evaluates path quality</li> <li>Various metric computation methods</li> </ul>"},{"location":"concepts/god/#usage-examples","title":"Usage Examples","text":""},{"location":"concepts/god/#basic-usage","title":"Basic Usage","text":"<pre><code>from god_llm.core import God\nfrom my_llm_implementation import MyLLM\n\n# Initialize\ngod = God(llm=MyLLM())\n\n# Generate initial response\nnode_id = god.expand(\"What is the nature of consciousness?\")\n\n# Get top reasoning paths\npaths = god.pray(k=3)\n\n# Generate readable summaries\nsummaries = god.miracle(k=3)\n</code></pre>"},{"location":"concepts/god/#with-debug-logging","title":"With Debug Logging","text":"<pre><code>god = God(\n    llm=MyLLM(),\n    debug=True,\n    similarity_threshold=0.8,\n    max_iterations=5\n)\n</code></pre>"},{"location":"concepts/god/#custom-template-manager","title":"Custom Template Manager","text":"<pre><code>from god_llm.templates import TemplateManager\n\ntemplates = TemplateManager()\ntemplates.add_template(\"custom_prompt\", \"...\")\n\ngod = God(llm=MyLLM(), template_manager=templates)\n</code></pre>"},{"location":"concepts/god/#best-practices","title":"Best Practices","text":"<ol> <li>Configuration</li> <li>Adjust <code>similarity_threshold</code> based on desired relationship strictness</li> <li>Set <code>max_iterations</code> based on needed exploration depth</li> <li> <p>Configure <code>context_window</code> based on memory requirements</p> </li> <li> <p>Performance</p> </li> <li>Monitor node count with <code>max_nodes</code> parameter</li> <li>Enable debug logging for troubleshooting</li> <li> <p>Use appropriate batch sizes for operations</p> </li> <li> <p>Quality Control</p> </li> <li>Implement retry logic for failed generations</li> <li>Monitor and handle irrelevant nodes</li> <li>Validate generated questions</li> </ol>"},{"location":"concepts/god/#error-handling","title":"Error Handling","text":"<p>The God class implements various error handling mechanisms: - Retry logic for failed expansions - Node relevance checking - Question quality filtering - Context validation</p>"},{"location":"concepts/god/#limitations","title":"Limitations","text":"<ol> <li>Memory Usage</li> <li>Node count grows with exploration depth</li> <li>Context history affects memory consumption</li> <li> <p>Large paths can impact performance</p> </li> <li> <p>Performance</p> </li> <li>Path finding complexity grows with node count</li> <li>Multiple LLM calls per expansion</li> <li> <p>Similarity computations can be intensive</p> </li> <li> <p>Quality Control</p> </li> <li>Depends on LLM quality</li> <li>Requires careful threshold tuning</li> <li>May need domain-specific adjustments</li> </ol>"},{"location":"concepts/god/#implementation-tips","title":"Implementation Tips","text":"<ol> <li> <p>Monitor system resources:    <pre><code>god = God(debug=True)\ngod.expand(\"prompt\", max_nodes=20)  # Limit node count\n</code></pre></p> </li> <li> <p>Implement cleanup strategies:    <pre><code># Regular cleanup of old nodes\nfor node_id in list(god.nodes.keys()):\n    if god.nodes[node_id].age &gt; max_age:\n        god._delete_node(node_id)\n</code></pre></p> </li> <li> <p>Use batch processing for large operations:    <pre><code># Process in batches\nresults = []\nfor batch in chunks(prompts, batch_size):\n    results.extend(god.expand(prompt) for prompt in batch)\n</code></pre></p> </li> </ol>"},{"location":"concepts/nodes/","title":"Nodes Documentation","text":""},{"location":"concepts/nodes/#overview","title":"Overview","text":"<p>The Node system forms the fundamental data structure of the God LLM framework. Each node represents a thought or response in the reasoning chain, maintaining relationships and context with other nodes.</p>"},{"location":"concepts/nodes/#node-class-structure","title":"Node Class Structure","text":""},{"location":"concepts/nodes/#core-attributes","title":"Core Attributes","text":"<pre><code>@dataclass\nclass Node:\n    prompt: Union[UserMessage, AIMessage]\n    thought: AIMessage\n    parent_id: Optional[str] = None\n    context_history: List[Context] = field(default_factory=list)\n    children: List[str] = field(default_factory=list)\n    relations: List[str] = field(default_factory=list)\n    id: str = field(default_factory=lambda: str(uuid4()))\n    score: float = 1.0\n</code></pre>"},{"location":"concepts/nodes/#context-class","title":"Context Class","text":"<pre><code>@dataclass\nclass Context:\n    prompt: str\n    thought: str\n    depth: int\n    node_id: str\n</code></pre>"},{"location":"concepts/nodes/#node-relationships","title":"Node Relationships","text":""},{"location":"concepts/nodes/#hierarchical-relationships","title":"Hierarchical Relationships","text":"<ol> <li>Parent-Child</li> <li>Each node has one optional parent</li> <li>Nodes can have multiple children</li> <li> <p>Forms tree-like structure</p> </li> <li> <p>Context History</p> </li> <li>Maintains list of ancestor contexts</li> <li>Includes prompts and thoughts</li> <li> <p>Records depth information</p> </li> <li> <p>Lateral Relationships</p> </li> <li>Based on semantic similarity</li> <li>Bidirectional connections</li> <li>Cross-branch relationships</li> </ol>"},{"location":"concepts/nodes/#node-operations","title":"Node Operations","text":""},{"location":"concepts/nodes/#creation","title":"Creation","text":"<pre><code># Basic node creation\nnode = Node(\n    prompt=UserMessage(content=\"What is consciousness?\"),\n    thought=AIMessage(content=\"Consciousness is...\"),\n)\n\n# With context\nnode = Node(\n    prompt=prompt_msg,\n    thought=thought_msg,\n    context_history=context_list,\n    parent_id=parent_node_id\n)\n</code></pre>"},{"location":"concepts/nodes/#relationship-management","title":"Relationship Management","text":"<pre><code># Add child\nparent_node.children.append(child_node.id)\n\n# Add relation\nnode1.relations.append(node2.id)\nnode2.relations.append(node1.id)\n</code></pre>"},{"location":"concepts/nodes/#context-building","title":"Context Building","text":"<pre><code># Build context history\ncontexts = []\ncurrent_id = node_id\nwhile current_id and len(contexts) &lt; max_depth:\n    node = nodes[current_id]\n    contexts.append(Context(\n        prompt=node.prompt.content,\n        thought=node.thought.content,\n        depth=len(contexts),\n        node_id=node.id\n    ))\n    current_id = node.parent_id\n</code></pre>"},{"location":"concepts/nodes/#node-scoring","title":"Node Scoring","text":""},{"location":"concepts/nodes/#base-score","title":"Base Score","text":"<ul> <li>Initial score of 1.0</li> <li>Modified based on various factors</li> <li>Used in path evaluation</li> </ul>"},{"location":"concepts/nodes/#context-retention-score","title":"Context Retention Score","text":"<pre><code>def _compute_context_retention_score(self) -&gt; float:\n    if not self.context_history:\n        return 1.0\n\n    # Compute based on context utilization\n    context_scores = []\n    for ctx in self.context_history:\n        relevance = compute_relevance(ctx, self.thought)\n        context_scores.append(relevance)\n\n    return np.mean(context_scores)\n</code></pre>"},{"location":"concepts/nodes/#best-practices","title":"Best Practices","text":"<ol> <li>Node Creation</li> <li>Include comprehensive context</li> <li>Set appropriate relationships</li> <li> <p>Validate inputs</p> </li> <li> <p>Relationship Management</p> </li> <li>Maintain bidirectional relations</li> <li>Update parent-child links</li> <li> <p>Clean up orphaned nodes</p> </li> <li> <p>Context Handling</p> </li> <li>Limit context history depth</li> <li>Include relevant context only</li> <li>Maintain context order</li> </ol>"},{"location":"concepts/nodes/#implementation-examples","title":"Implementation Examples","text":""},{"location":"concepts/nodes/#basic-node-usage","title":"Basic Node Usage","text":"<pre><code># Create root node\nroot = Node(\n    prompt=UserMessage(\"Initial question\"),\n    thought=AIMessage(\"Initial response\")\n)\n\n# Add child node\nchild = Node(\n    prompt=UserMessage(\"Follow-up question\"),\n    thought=AIMessage(\"Follow-up response\"),\n    parent_id=root.id,\n    context_history=[Context(\n        prompt=root.prompt.content,\n        thought=root.thought.content,\n        depth=0,\n        node_id=root.id\n    )]\n)\n\n# Set relationships\nroot.children.append(child.id)\n</code></pre>"},{"location":"concepts/nodes/#context-management","title":"Context Management","text":"<pre><code>def build_context(node: Node, max_depth: int = 3) -&gt; List[Context]:\n    contexts = []\n    current = node\n    depth = 0\n\n    while current.parent_id and depth &lt; max_depth:\n        parent = nodes[current.parent_id]\n        contexts.append(Context(\n            prompt=parent.prompt.content,\n            thought=parent.thought.content,\n            depth=depth,\n            node_id=parent.id\n        ))\n        current = parent\n        depth += 1\n\n    return list(reversed(contexts))\n</code></pre>"},{"location":"concepts/nodes/#relationship-building","title":"Relationship Building","text":"<pre><code>def build_relations(node: Node, nodes: Dict[str, Node], threshold: float = 0.7):\n    for other_id, other_node in nodes.items():\n        if other_id != node.id:\n            similarity = compute_similarity(node, other_node)\n            if similarity &gt; threshold:\n                node.relations.append(other_id)\n                other_node.relations.append(node.id)\n</code></pre>"},{"location":"concepts/nodes/#error-handling","title":"Error Handling","text":"<ol> <li> <p>Node Validation <pre><code>def validate_node(node: Node) -&gt; bool:\n    if not node.prompt or not node.thought:\n        return False\n    if node.parent_id and node.parent_id not in nodes:\n        return False\n    return True\n</code></pre></p> </li> <li> <p>Relationship Cleanup <pre><code>def cleanup_relations(nodes: Dict[str, Node]):\n    for node in nodes.values():\n        node.relations = [r for r in node.relations if r in nodes]\n</code></pre></p> </li> <li> <p>Context Validation <pre><code>def validate_context(context: List[Context]) -&gt; bool:\n    if not context:\n        return True\n    depths = [ctx.depth for ctx in context]\n    return depths == sorted(depths)\n</code></pre></p> </li> </ol>"},{"location":"concepts/nodes/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Memory Management</li> <li>Implement node cleanup strategies</li> <li>Limit context history size</li> <li> <p>Monitor relationship growth</p> </li> <li> <p>Computation Efficiency</p> </li> <li>Cache similarity calculations</li> <li>Batch relationship updates</li> <li> <p>Optimize context building</p> </li> <li> <p>Storage Optimization</p> </li> <li>Implement node serialization</li> <li>Manage context history size</li> <li>Clean up unused relationships</li> </ol>"},{"location":"concepts/overview/","title":"Core Concepts Overview","text":""},{"location":"concepts/overview/#introduction","title":"Introduction","text":"<p>The God LLM system is an advanced framework for enabling Large Language Models (LLMs) to perform deep contextual reasoning and knowledge exploration. Drawing inspiration from Proverbs 2:6, it aims to provide wisdom and understanding through structured thought processes.</p>"},{"location":"concepts/overview/#key-components","title":"Key Components","text":""},{"location":"concepts/overview/#1-node-based-knowledge-structure","title":"1. Node-Based Knowledge Structure","text":"<p>The system organizes knowledge and reasoning in a graph-like structure where: - Each node represents a thought or response to a prompt - Nodes maintain parent-child relationships - Nodes can have lateral relationships with other nodes based on semantic similarity - Each node contains:   - The original prompt   - The LLM's thought/response   - Context history   - Relationships to other nodes</p>"},{"location":"concepts/overview/#2-scoring-metrics","title":"2. Scoring Metrics","text":"<p>The system evaluates reasoning paths using multiple sophisticated metrics:</p> <ul> <li>Depth Score: Measures how deep and thorough the reasoning is</li> <li>Relation Score: Evaluates lateral connections between nodes</li> <li>Coherence Score: Assesses logical flow and consistency between connected thoughts</li> <li>Novelty Score: Measures uniqueness of insights compared to existing nodes</li> <li>Context Retention: Evaluates how well context is maintained throughout the reasoning chain</li> <li>Hierarchy Score: Assesses the strength of parent-child relationships in the reasoning path</li> </ul>"},{"location":"concepts/overview/#3-core-operations","title":"3. Core Operations","text":""},{"location":"concepts/overview/#expand-operation","title":"Expand Operation","text":"<ul> <li>Takes a prompt and generates a thoughtful response</li> <li>Creates follow-up questions to explore the topic deeper</li> <li>Builds a tree of interconnected thoughts</li> <li>Maintains context awareness through previous interactions</li> <li>Implements relevance checking and retry mechanisms</li> </ul>"},{"location":"concepts/overview/#pray-operation","title":"Pray Operation","text":"<ul> <li>Discovers and evaluates all possible paths through the knowledge graph</li> <li>Computes comprehensive scores for each path</li> <li>Returns the top-k paths based on scoring metrics</li> <li>Enables identification of the most valuable reasoning chains</li> </ul>"},{"location":"concepts/overview/#miracle-operation","title":"Miracle Operation","text":"<ul> <li>Builds upon the Pray operation</li> <li>Generates human-readable summaries of the best reasoning paths</li> <li>Can output results in Markdown format for documentation</li> <li>Provides detailed metrics and analysis of each path</li> </ul>"},{"location":"concepts/overview/#4-context-management","title":"4. Context Management","text":"<ul> <li>Maintains a sliding window of previous interactions</li> <li>Builds enhanced prompts that include relevant context</li> <li>Ensures coherence and continuity in reasoning chains</li> <li>Implements similarity-based relationship discovery</li> </ul>"},{"location":"concepts/overview/#5-quality-control","title":"5. Quality Control","text":"<ul> <li>Evaluates and filters generated questions for quality</li> <li>Implements retry mechanisms for irrelevant responses</li> <li>Maintains similarity thresholds for relationship building</li> <li>Provides comprehensive debugging and logging capabilities</li> </ul>"},{"location":"concepts/overview/#key-features","title":"Key Features","text":""},{"location":"concepts/overview/#contextual-awareness","title":"Contextual Awareness","text":"<ul> <li>Maintains history of previous interactions</li> <li>Builds relationships between related thoughts</li> <li>Ensures coherence in reasoning chains</li> </ul>"},{"location":"concepts/overview/#quality-assessment","title":"Quality Assessment","text":"<ul> <li>Multiple scoring metrics for evaluation</li> <li>Filtering mechanisms for generated questions</li> <li>Relevance checking for responses</li> </ul>"},{"location":"concepts/overview/#flexibility","title":"Flexibility","text":"<ul> <li>Configurable parameters for fine-tuning</li> <li>Template-based prompt generation</li> <li>Extensible scoring system</li> </ul>"},{"location":"concepts/overview/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive logging capabilities</li> <li>Markdown export functionality</li> <li>Detailed metrics and analysis</li> </ul>"},{"location":"concepts/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Configuration</li> <li>Set appropriate similarity thresholds based on your use case</li> <li>Adjust context window size based on memory requirements</li> <li> <p>Configure max iterations based on desired exploration depth</p> </li> <li> <p>Usage</p> </li> <li>Start with clear, well-defined prompts</li> <li>Use the miracle operation for generating readable summaries</li> <li> <p>Enable debugging for detailed insight into system operation</p> </li> <li> <p>Performance</p> </li> <li>Monitor node count to prevent excessive memory usage</li> <li>Use appropriate batch sizes for question generation</li> <li>Consider context window size impact on performance</li> </ol>"},{"location":"concepts/overview/#implementation-considerations","title":"Implementation Considerations","text":"<ol> <li>Memory Management</li> <li>Implement node cleanup for irrelevant branches</li> <li>Monitor and limit maximum node count</li> <li> <p>Consider context window size impact</p> </li> <li> <p>Error Handling</p> </li> <li>Implement retry mechanisms for failed generations</li> <li>Handle edge cases in path exploration</li> <li> <p>Maintain fallback strategies for scoring failures</p> </li> <li> <p>Scalability</p> </li> <li>Consider batch processing for large operations</li> <li>Implement efficient path finding algorithms</li> <li>Optimize similarity computations</li> </ol>"},{"location":"concepts/overview/#further-reading","title":"Further Reading","text":"<ul> <li>Large Language Models and Contextual Reasoning</li> <li>Graph-Based Knowledge Representation</li> <li>Semantic Similarity in NLP</li> <li>Effective Prompt Engineering</li> </ul>"},{"location":"concepts/scoring/","title":"Scoring Documentation","text":""},{"location":"concepts/scoring/#overview","title":"Overview","text":"<p>The scoring system in God LLM provides comprehensive evaluation of reasoning paths through multiple metrics. This sophisticated scoring approach ensures high-quality, coherent, and meaningful knowledge exploration.</p>"},{"location":"concepts/scoring/#scoring-metrics","title":"Scoring Metrics","text":""},{"location":"concepts/scoring/#scoremetric-enum","title":"ScoreMetric Enum","text":"<pre><code>class ScoreMetric(Enum):\n    DEPTH = \"depth\"\n    RELATION = \"relation\"\n    COHERENCE = \"coherence\"\n    NOVELTY = \"novelty\"\n    CONTEXT_RETENTION = \"context_retention\"\n    HIERARCHY = \"hierarchy\"\n</code></pre>"},{"location":"concepts/scoring/#metric-weights","title":"Metric Weights","text":"<pre><code>metric_weights = {\n    ScoreMetric.DEPTH: 0.25,\n    ScoreMetric.RELATION: 0.15,\n    ScoreMetric.COHERENCE: 0.25,\n    ScoreMetric.NOVELTY: 0.15,\n    ScoreMetric.CONTEXT_RETENTION: 0.20,\n    ScoreMetric.HIERARCHY: 0.20\n}\n</code></pre>"},{"location":"concepts/scoring/#scoring-components","title":"Scoring Components","text":""},{"location":"concepts/scoring/#1-depth-score","title":"1. Depth Score","text":"<p>Evaluates the depth and thoroughness of reasoning: <pre><code>def _compute_depth_score(self, path: List[str]) -&gt; float:\n    return np.mean([self.nodes[node_id].score for node_id in path])\n</code></pre></p>"},{"location":"concepts/scoring/#2-relation-score","title":"2. Relation Score","text":"<p>Measures lateral connections between nodes: <pre><code>def _compute_relation_score(self, path: List[str]) -&gt; float:\n    if len(path) &lt; 2:\n        return 1.0\n\n    relation_scores = []\n    for node_id in path:\n        node = self.nodes[node_id]\n        related_nodes = set(node.relations).intersection(set(path))\n        potential_relations = len(path) - 1\n        score = len(related_nodes) / potential_relations if potential_relations &gt; 0 else 0\n        relation_scores.append(score)\n\n    return np.mean(relation_scores)\n</code></pre></p>"},{"location":"concepts/scoring/#3-coherence-score","title":"3. Coherence Score","text":"<p>Evaluates logical flow and consistency: <pre><code>def _compute_coherence_score(self, path: List[str]) -&gt; float:\n    if len(path) &lt; 2:\n        return 1.0\n\n    coherence_scores = []\n    for i in range(len(path) - 1):\n        current_node = self.nodes[path[i]]\n        next_node = self.nodes[path[i + 1]]\n        similarity = self._compute_similarity(current_node, next_node)\n        coherence_scores.append(similarity)\n\n    return np.mean(coherence_scores)\n</code></pre></p>"},{"location":"concepts/scoring/#4-novelty-score","title":"4. Novelty Score","text":"<p>Assesses uniqueness of insights: <pre><code>def _compute_novelty_score(self, path: List[str]) -&gt; float:\n    path_nodes = [self.nodes[node_id] for node_id in path]\n    other_nodes = [node for node_id, node in self.nodes.items() \n                   if node_id not in path]\n\n    if not other_nodes:\n        return 0.5\n\n    novelty_scores = []\n    for path_node in path_nodes:\n        similarities = [self._compute_similarity(path_node, other_node) \n                      for other_node in other_nodes]\n        novelty_scores.append(1 - np.mean(similarities))\n\n    return np.mean(novelty_scores)\n</code></pre></p>"},{"location":"concepts/scoring/#5-context-retention-score","title":"5. Context Retention Score","text":"<p>Measures maintenance of context: <pre><code>def _compute_context_retention_score(self, path: List[str]) -&gt; float:\n    if len(path) &lt; 2:\n        return 1.0\n\n    retention_scores = []\n    for i in range(1, len(path)):\n        current_node = self.nodes[path[i]]\n        context_score = current_node._compute_context_retention_score()\n        retention_scores.append(context_score)\n\n    return np.mean(retention_scores)\n</code></pre></p>"},{"location":"concepts/scoring/#6-hierarchy-score","title":"6. Hierarchy Score","text":"<p>Evaluates strength of hierarchical relationships: <pre><code>def _compute_hierarchy_score(self, path: List[str]) -&gt; float:\n    if len(path) &lt; 2:\n        return 1.0\n</code></pre></p>"},{"location":"cookbook/advanced/","title":"Advanced Patterns Cookbook","text":"<p>Write content for Advanced Patterns Cookbook here.</p>"},{"location":"cookbook/advanced/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"cookbook/advanced/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"cookbook/basic/","title":"God Module Cookbook - Basic Examples","text":"<p>A collection of practical examples showing common use patterns of the God Module.</p>"},{"location":"cookbook/basic/#1-simple-question-exploration","title":"1. Simple Question Exploration","text":"<pre><code>from god_llm.core import God\nfrom god_llm.plugins.groq import ChatGroq\n\n# Initialize\ngod = God(llm=ChatGroq(api_key=\"you_api_key\", model_name=\"model_name\"))\n\n# Basic exploration\nnode_id = god.expand(\"What are the implications of quantum computing?\")\n\n# Get paths\npaths = god.pray(k=3)\n\n# Print summaries\nfor path in god.miracle(k=3)[\"paths\"]:\n    print(f\"Path score: {path['score']}\")\n    print(f\"Summary: {path['summary']}\\n\")\n</code></pre>"},{"location":"cookbook/basic/#2-connected-reasoning-chains","title":"2. Connected Reasoning Chains","text":"<pre><code># Start with a root question\nroot_id = god.expand(\"What are the environmental impacts of AI?\")\n\n# Follow up on power consumption\nenergy_id = god.expand(\n    \"How does AI training affect energy consumption?\",\n    parent_id=root_id\n)\n\n# Explore solutions\nsolutions_id = god.expand(\n    \"What are potential solutions for reducing AI's energy footprint?\",\n    parent_id=energy_id\n)\n\n# Get the complete reasoning chain\npaths = god.pray(k=1)\n</code></pre>"},{"location":"cookbook/basic/#3-comparative-analysis","title":"3. Comparative Analysis","text":"<pre><code>def compare_topics(god: God, topic1: str, topic2: str):\n    # Analyze first topic\n    id1 = god.expand(f\"Analyze {topic1}\")\n\n    # Analyze second topic\n    id2 = god.expand(f\"Analyze {topic2}\")\n\n    # Compare both\n    comparison_id = god.expand(\n        f\"Compare and contrast {topic1} and {topic2}\",\n        parent_id=id1  # Connect to first topic\n    )\n\n    # Get insights\n    return god.miracle(k=1)\n\n# Usage\nresults = compare_topics(\n    god,\n    \"renewable energy\",\n    \"nuclear power\"\n)\n</code></pre>"},{"location":"cookbook/basic/#4-deep-dive-analysis","title":"4. Deep Dive Analysis","text":"<pre><code>def deep_dive(god: God, topic: str, depth: int = 3):\n    # Initial exploration\n    current_id = god.expand(topic)\n\n    # Deeper levels\n    for level in range(depth):\n        # Get current paths\n        paths = god.pray(k=1)\n\n        # Find most interesting question\n        current_path = paths[\"paths\"][0]\n        next_question = current_path[\"questions\"][0]\n\n        # Explore further\n        current_id = god.expand(\n            next_question,\n            parent_id=current_id\n        )\n\n    return god.miracle(k=1)\n\n# Usage\nanalysis = deep_dive(\n    god,\n    \"How does artificial intelligence impact privacy?\",\n    depth=3\n)\n</code></pre>"},{"location":"cookbook/basic/#5-topic-mapping","title":"5. Topic Mapping","text":"<pre><code>def map_related_topics(god: God, central_topic: str, num_branches: int = 3):\n    # Central node\n    center_id = god.expand(central_topic)\n\n    # Create branches\n    branch_ids = []\n    for _ in range(num_branches):\n        # Get current state\n        paths = god.pray(k=1)\n\n        # Pick a question\n        question = paths[\"paths\"][0][\"questions\"][0]\n\n        # Create branch\n        branch_id = god.expand(\n            question,\n            parent_id=center_id\n        )\n        branch_ids.append(branch_id)\n\n    return god.miracle(k=num_branches)\n\n# Usage\ntopic_map = map_related_topics(\n    god,\n    \"What is machine learning?\",\n    num_branches=3\n)\n</code></pre>"},{"location":"cookbook/basic/#6-iterative-refinement","title":"6. Iterative Refinement","text":"<pre><code>def refine_analysis(god: God, topic: str, iterations: int = 3):\n    current_id = god.expand(topic)\n\n    for i in range(iterations):\n        # Get current insights\n        paths = god.pray(k=1)\n\n        # Generate refinement prompt\n        refinement = f\"Refine and expand on: {paths['paths'][0]['summary']}\"\n\n        # Create refined analysis\n        current_id = god.expand(\n            refinement,\n            parent_id=current_id\n        )\n\n    return god.miracle(k=1)\n\n# Usage\nrefined_analysis = refine_analysis(\n    god,\n    \"What are the ethical implications of AI?\",\n    iterations=3\n)\n</code></pre>"},{"location":"cookbook/basic/#7-batch-processing","title":"7. Batch Processing","text":"<pre><code>def batch_analyze(god: God, topics: List[str], max_nodes_per_topic: int = 5):\n    results = {}\n\n    for topic in topics:\n        # Analyze with node limit\n        node_id = god.expand(\n            topic,\n            max_nodes=max_nodes_per_topic\n        )\n\n        # Get summary\n        summary = god.miracle(k=1)\n        results[topic] = summary\n\n    return results\n\n# Usage\ntopics = [\n    \"Climate change solutions\",\n    \"Future of transportation\",\n    \"Space exploration\"\n]\n\nbatch_results = batch_analyze(god, topics)\n</code></pre>"},{"location":"cookbook/basic/#8-error-handling","title":"8. Error Handling","text":"<pre><code>def safe_expansion(god: God, prompt: str, max_retries: int = 3):\n    for attempt in range(max_retries):\n        try:\n            node_id = god.expand(\n                prompt,\n                retry_count=attempt\n            )\n            return node_id\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Retry {attempt + 1}/{max_retries}: {str(e)}\")\n\n    return None\n\n# Usage\ntry:\n    node_id = safe_expansion(\n        god,\n        \"Complex analysis prompt\"\n    )\nexcept Exception as e:\n    print(f\"Analysis failed: {str(e)}\")\n</code></pre>"},{"location":"cookbook/integration/","title":"Integration Examples Cookbook","text":"<p>Write content for Integration Examples Cookbook here.</p>"},{"location":"cookbook/integration/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"cookbook/integration/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>Learn how to configure and customize the God Module for your specific needs.</p>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"getting-started/configuration/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>from god_llm.core import God\nfrom god_llm.templates import TemplateManager\nfrom god_llm.scoring import MinQuestionScore\n\ngod = God(\n    llm=YourLLM(),                    # Required: Language model interface\n    similarity_threshold=0.7,          # Optional: Default 0.7\n    max_iterations=4,                  # Optional: Default 4\n    context_window=3,                  # Optional: Default 3\n    min_question_score=MinQuestionScore(min_question_score=0.4),  # Optional\n    template_manager=None,             # Optional: Custom template manager\n    debug=False                        # Optional: Default False\n)\n</code></pre>"},{"location":"getting-started/configuration/#parameter-details","title":"Parameter Details","text":""},{"location":"getting-started/configuration/#1-language-model-llm","title":"1. Language Model (<code>llm</code>)","text":"<pre><code># Example custom LLM implementation\nfrom god_llm.llm import BaseLLM\n\nclass CustomLLM(BaseLLM):\n    def generate(self, prompt: str) -&gt; str:\n        # Your implementation here\n        pass\n\n    def get_embedding(self, text: str) -&gt; List[float]:\n        # Your implementation here\n        pass\n\ngod = God(llm=CustomLLM())\n</code></pre>"},{"location":"getting-started/configuration/#2-similarity-threshold-similarity_threshold","title":"2. Similarity Threshold (<code>similarity_threshold</code>)","text":"<pre><code># Higher threshold for stricter relationship matching\ngod = God(\n    llm=YourLLM(),\n    similarity_threshold=0.85  # Only very similar nodes will be connected\n)\n\n# Lower threshold for more flexible connections\ngod = God(\n    llm=YourLLM(),\n    similarity_threshold=0.6   # More nodes will be considered related\n)\n</code></pre>"},{"location":"getting-started/configuration/#3-maximum-iterations-max_iterations","title":"3. Maximum Iterations (<code>max_iterations</code>)","text":"<pre><code># Deep exploration\ngod = God(\n    llm=YourLLM(),\n    max_iterations=6    # Allow deeper reasoning chains\n)\n\n# Shallow exploration\ngod = God(\n    llm=YourLLM(),\n    max_iterations=2    # Keep reasoning chains short\n)\n</code></pre>"},{"location":"getting-started/configuration/#4-context-window-context_window","title":"4. Context Window (<code>context_window</code>)","text":"<pre><code># Large context window\ngod = God(\n    llm=YourLLM(),\n    context_window=5    # Keep more previous nodes in context\n)\n\n# Minimal context\ngod = God(\n    llm=YourLLM(),\n    context_window=1    # Only use immediate parent node\n)\n</code></pre>"},{"location":"getting-started/configuration/#5-minimum-question-score-min_question_score","title":"5. Minimum Question Score (<code>min_question_score</code>)","text":"<pre><code># Strict question filtering\ngod = God(\n    llm=YourLLM(),\n    min_question_score=MinQuestionScore(min_question_score=0.7)  # Only high-quality questions\n)\n\n# Lenient question filtering\ngod = God(\n    llm=YourLLM(),\n    min_question_score=MinQuestionScore(min_question_score=0.3)  # Accept more questions\n)\n</code></pre>"},{"location":"getting-started/configuration/#6-template-manager-template_manager","title":"6. Template Manager (<code>template_manager</code>)","text":"<pre><code># Custom templates\ntemplate_manager = TemplateManager()\n\n# Add custom prompts\ntemplate_manager.add_template(\n    \"custom_expansion\",\n    \"Given the context '{context}', analyze the following: {prompt}\"\n)\n\ntemplate_manager.add_template(\n    \"custom_question\",\n    \"Generate relevant questions about: {topic}\"\n)\n\ngod = God(\n    llm=YourLLM(),\n    template_manager=template_manager\n)\n</code></pre>"},{"location":"getting-started/configuration/#7-debug-mode-debug","title":"7. Debug Mode (<code>debug</code>)","text":"<pre><code># Enable detailed logging\ngod = God(\n    llm=YourLLM(),\n    debug=True\n)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-profiles","title":"Configuration Profiles","text":""},{"location":"getting-started/configuration/#deep-analysis-profile","title":"Deep Analysis Profile","text":"<pre><code>god = God(\n    llm=YourLLM(),\n    similarity_threshold=0.8,\n    max_iterations=6,\n    context_window=4,\n    min_question_score=MinQuestionScore(min_question_score=0.6),\n    debug=True\n)\n</code></pre>"},{"location":"getting-started/configuration/#quick-exploration-profile","title":"Quick Exploration Profile","text":"<pre><code>god = God(\n    llm=YourLLM(),\n    similarity_threshold=0.6,\n    max_iterations=2,\n    context_window=1,\n    min_question_score=MinQuestionScore(min_question_score=0.4),\n    debug=False\n)\n</code></pre>"},{"location":"getting-started/configuration/#memory-optimized-profile","title":"Memory-Optimized Profile","text":"<pre><code>god = God(\n    llm=YourLLM(),\n    similarity_threshold=0.7,\n    max_iterations=3,\n    context_window=2,\n    min_question_score=MinQuestionScore(min_question_score=0.5),\n    debug=False\n)\n</code></pre>"},{"location":"getting-started/configuration/#runtime-configuration","title":"Runtime Configuration","text":""},{"location":"getting-started/configuration/#expand-method-parameters","title":"Expand Method Parameters","text":"<pre><code># Configure individual expand operations\nnode_id = god.expand(\n    prompt=\"Your prompt here\",\n    parent_id=\"optional_parent_id\",    # Connect to specific node\n    depth=0,                           # Starting depth\n    retry_count=2,                     # Number of retries\n    max_nodes=15                       # Node limit\n)\n</code></pre>"},{"location":"getting-started/configuration/#path-analysis-configuration","title":"Path Analysis Configuration","text":"<pre><code># Configure path analysis\npaths = god.pray(k=3)          # Get top 3 paths\nsummaries = god.miracle(k=5)   # Get top 5 summarized paths\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/configuration/#1-resource-management","title":"1. Resource Management","text":"<pre><code># For memory-intensive operations\ngod = God(\n    llm=YourLLM(),\n    context_window=2,          # Smaller context\n    max_iterations=3,          # Limited depth\n    debug=True                # Monitor resource usage\n)\n</code></pre>"},{"location":"getting-started/configuration/#2-quality-optimization","title":"2. Quality Optimization","text":"<pre><code># For high-quality results\ngod = God(\n    llm=YourLLM(),\n    similarity_threshold=0.85,\n    min_question_score=MinQuestionScore(min_question_score=0.7),\n    max_iterations=5\n)\n</code></pre>"},{"location":"getting-started/configuration/#3-performance-optimization","title":"3. Performance Optimization","text":"<pre><code># For faster processing\ngod = God(\n    llm=YourLLM(),\n    context_window=1,\n    max_iterations=2,\n    debug=False\n)\n</code></pre>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#common-issues","title":"Common Issues","text":"<ol> <li>High Memory Usage:</li> <li>Reduce <code>context_window</code></li> <li>Lower <code>max_iterations</code></li> <li> <p>Implement regular cleanup</p> </li> <li> <p>Poor Quality Results:</p> </li> <li>Increase <code>similarity_threshold</code></li> <li>Raise <code>min_question_score</code></li> <li> <p>Use custom templates</p> </li> <li> <p>Slow Performance:</p> </li> <li>Reduce <code>max_iterations</code></li> <li>Lower <code>context_window</code></li> <li>Batch process with limits</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#quick-start","title":"Quick Start","text":"<p>You can install god_llm directly from PyPI:</p> <pre><code>pip install god_llm\n</code></pre>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing god_llm, ensure you have: - Python 3.8 or higher - pip (Python package installer) - A compatible LLM API key (e.g., Groq, OpenAI, Anthropic)</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>The simplest way to install god_llm is through PyPI:</p> <pre><code>pip install god_llm\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/MR-GREEN1337/god_llm.git\ncd god_llm\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you plan to contribute to god_llm, install with development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>god_llm supports multiple LLM providers. Here's how to set up each one:</p>"},{"location":"getting-started/installation/#groq","title":"Groq","text":"<ol> <li>Get your API key from Groq's platform</li> <li>Set up your environment:    <pre><code>export GROQ_API_KEY=\"your-api-key-here\"\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#installing-optional-dependencies","title":"Installing Optional Dependencies","text":"<p>For additional features, install the relevant extras:</p> <pre><code># For visualization support\npip install \"god_llm[viz]\"\n\n# For all optional features\npip install \"god_llm[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>from god_llm.core.god import God\nfrom god_llm.plugins.groq import ChatGroq\n\n# Initialize with Groq\nllm = ChatGroq(\n    model_name=\"llama-3.1-70b-versatile\",\n    api_key=\"your-api-key\"  # Or use environment variable\n)\n\n# Create God instance\ngod = God(llm=llm)\n\n# Test the installation\nresult = god.expand(\"What is the meaning of life?\")\nprint(\"Installation successful!\" if result else \"Installation failed!\")\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#api-key-not-found","title":"API Key Not Found","text":"<p><pre><code>ApiKeyError: API key not found\n</code></pre> Solution: Ensure you've properly set your API key either in the environment or when initializing the LLM.</p>"},{"location":"getting-started/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p><pre><code>ImportError: Cannot import name 'X' from 'Y'\n</code></pre> Solution: Try creating a fresh virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install god_llm\n</code></pre></p>"},{"location":"getting-started/installation/#overview","title":"Overview","text":"<p>god_llm is a framework for recursive thought expansion using Large Language Models. It enables deep exploration of ideas while maintaining context and relevance. The package includes:</p> <ul> <li>Core functionality for thought expansion</li> <li>Multiple LLM provider integrations</li> <li>Visualization tools</li> <li>Advanced scoring and path finding</li> </ul>"},{"location":"getting-started/installation/#further-reading","title":"Further Reading","text":"<ul> <li>Official Documentation</li> <li>Let's Build a God LLM - Original concept article</li> <li>Contributing Guidelines</li> <li>API Reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get started with the God Module, a contextual reasoning system powered by Large Language Models.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install god_llm\n</code></pre>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-initialize-the-god-module","title":"1. Initialize the God Module","text":"<pre><code>from god_llm.core import God\nfrom god_llm.plugins.groq import ChatGroq\n\n# Initialize with default settings\nllm = ChatGroq(\n    model_name=\"llama-3.1-70b-versatile\",\n    api_key=\"\", #Dummy API key\n)\ngod = God(llm=llm)\n</code></pre>"},{"location":"getting-started/quickstart/#2-generate-initial-thoughts","title":"2. Generate Initial Thoughts","text":"<pre><code># Create your first node\nnode_id = god.expand(\"What is the role of artificial intelligence in healthcare?\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-discover-reasoning-paths","title":"3. Discover Reasoning Paths","text":"<pre><code># Get top 3 reasoning paths\npaths = god.pray(k=3)\n\n# Generate human-readable summaries\nsummaries = god.miracle(k=3)\n</code></pre>"},{"location":"getting-started/quickstart/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more control over the system's behavior:</p> <pre><code>god = God(\n    llm=YourLLM(),\n    similarity_threshold=0.8,  # Higher threshold for stricter relationships\n    max_iterations=5,         # Deeper exploration\n    context_window=4,         # Larger context history\n    debug=True               # Enable detailed logging\n)\n</code></pre>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quickstart/#1-sequential-reasoning","title":"1. Sequential Reasoning","text":"<pre><code># Start with an initial question\nroot_id = god.expand(\"What are the environmental impacts of renewable energy?\")\n\n# Explore a specific aspect\nfollow_up_id = god.expand(\"How does battery production affect sustainability?\", parent_id=root_id)\n\n# Get the reasoning chain\npaths = god.pray(k=1)\n</code></pre>"},{"location":"getting-started/quickstart/#2-batch-processing","title":"2. Batch Processing","text":"<pre><code>questions = [\n    \"What are the ethical implications of AI?\",\n    \"How does quantum computing affect cybersecurity?\",\n    \"What role does blockchain play in finance?\"\n]\n\n# Process multiple questions\nnode_ids = [god.expand(q, max_nodes=5) for q in questions]\n</code></pre>"},{"location":"getting-started/quickstart/#3-quality-control","title":"3. Quality Control","text":"<pre><code>god = God(\n    llm=YourLLM(),\n    similarity_threshold=0.85  # Higher relevance requirement\n)\n\n# Expansion with retry logic\nnode_id = god.expand(\"Complex topic\", retry_count=2)\n</code></pre>"},{"location":"getting-started/quickstart/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Management</li> <li>Set <code>max_nodes</code> limit for large explorations</li> <li>Monitor system memory usage with debug mode</li> <li> <p>Use appropriate batch sizes for bulk operations</p> </li> <li> <p>Quality Optimization</p> </li> <li>Adjust similarity threshold based on your needs</li> <li>Configure minimum question scores for better relevance</li> <li> <p>Use context window size appropriate for your use case</p> </li> <li> <p>Error Handling</p> </li> <li>Implement try-except blocks for expansions</li> <li>Monitor debug logs for issues</li> <li>Clean up old nodes regularly</li> </ol>"},{"location":"getting-started/quickstart/#performance-tips","title":"Performance Tips","text":"<pre><code># Regular cleanup\ndef cleanup_old_nodes(god_instance, max_age):\n    for node_id in list(god_instance.nodes.keys()):\n        if god_instance.nodes[node_id].age &gt; max_age:\n            god_instance._delete_node(node_id)\n\n# Batch processing with resource limits\ndef process_batch(god_instance, prompts, batch_size=5):\n    results = []\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i + batch_size]\n        results.extend(god_instance.expand(p, max_nodes=10) for p in batch)\n    return results\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore advanced features in the full documentation</li> <li>Customize prompt templates for your use case</li> <li>Implement domain-specific quality controls</li> <li>Monitor and optimize system performance</li> </ul>"},{"location":"plugins/anthropic/","title":"Anthropic Integration","text":"<p>The <code>ChatAnthropic</code> class provides integration with Anthropic's language models through their official API. This implementation handles authentication, message formatting, and response processing specific to Anthropic's requirements.</p>"},{"location":"plugins/anthropic/#class-overview","title":"Class Overview","text":"<pre><code>class ChatAnthropic(BaseLLM):\n    def __init__(\n        self,\n        model_name: AnthropicModel,\n        temperature: Optional[TemperatureValidator] = 0.7,\n        api_key: Optional[str] = None,\n    )\n</code></pre>"},{"location":"plugins/anthropic/#parameters","title":"Parameters","text":"<ul> <li><code>model_name</code>: The specific Anthropic model to use (e.g., \"claude-3\")</li> <li><code>temperature</code>: Controls response randomness (0.0 to 1.0)</li> <li><code>api_key</code>: Optional API key for authentication</li> </ul>"},{"location":"plugins/anthropic/#methods","title":"Methods","text":""},{"location":"plugins/anthropic/#generateprompt-str-aimessage","title":"generate(prompt: str) -&gt; AIMessage","text":"<p>Generates a completion using the Anthropic API:</p> <ul> <li>Includes system message defining assistant behavior</li> <li>Handles message formatting according to Anthropic's requirements</li> <li>Returns structured response in <code>AIMessage</code> format</li> <li>Includes metadata about the generation settings</li> </ul>"},{"location":"plugins/anthropic/#error-handling","title":"Error Handling","text":"<p>Errors during generation are wrapped in <code>BaseLLMException</code> with detailed error messages.</p>"},{"location":"plugins/anthropic/#example-usage","title":"Example Usage","text":"<pre><code>from god_llm.plugins import ChatAnthropic\n\n# Initialize the client\nanthropic = ChatAnthropic(\n    model_name=\"claude-3\",\n    temperature=0.7\n)\n\n# Generate a response\ntry:\n    response = anthropic.generate(\"Explain quantum computing\")\n    print(response.content)\nexcept BaseLLMException as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"plugins/custom/","title":"Custom Plugins","text":"<p>Coming soon</p>"},{"location":"plugins/groq/","title":"Groq Integration","text":"<p>The <code>ChatGroq</code> class implements integration with Groq's language models. This implementation provides a standardized interface while handling Groq-specific requirements for API interaction.</p>"},{"location":"plugins/groq/#class-overview","title":"Class Overview","text":"<pre><code>class ChatGroq(BaseLLM):\n    def __init__(\n        self,\n        model_name: GroqModel,\n        temperature: Optional[TemperatureValidator] = 0.7,\n        api_key: Optional[str] = None,\n    )\n</code></pre>"},{"location":"plugins/groq/#parameters","title":"Parameters","text":"<ul> <li><code>model_name</code>: The specific Groq model to use</li> <li><code>temperature</code>: Controls response randomness (0.0 to 1.0)</li> <li><code>api_key</code>: Optional API key for authentication</li> </ul>"},{"location":"plugins/groq/#methods","title":"Methods","text":""},{"location":"plugins/groq/#generateprompt-str-aimessage","title":"generate(prompt: str) -&gt; AIMessage","text":"<p>Generates a completion using the Groq API:</p> <ul> <li>Formats messages according to Groq's specifications</li> <li>Includes system message for consistent behavior</li> <li>Returns structured response in <code>AIMessage</code> format</li> <li>Includes metadata about generation parameters</li> </ul>"},{"location":"plugins/groq/#error-handling","title":"Error Handling","text":"<p>All API errors are caught and wrapped in <code>BaseLLMException</code> for consistent error handling across the library.</p>"},{"location":"plugins/groq/#example-usage","title":"Example Usage","text":"<pre><code>from god_llm.plugins import ChatGroq\n\n# Initialize the client\ngroq = ChatGroq(\n    model_name=\"llama-3.1-70b-versatile\",\n    temperature=0.7\n)\n\n# Generate a response\ntry:\n    response = groq.generate(\"Explain machine learning\")\n    print(response.content)\nexcept BaseLLMException as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"plugins/openai/","title":"OpenAI Integration","text":"<p>The <code>ChatOpenAI</code> class provides integration with OpenAI's language models. This implementation maintains consistency with the library's interface while handling OpenAI-specific requirements.</p>"},{"location":"plugins/openai/#class-overview","title":"Class Overview","text":"<pre><code>class ChatOpenAI(BaseLLM):\n    def __init__(\n        self,\n        model_name: OpenAIModel,\n        temperature: Optional[TemperatureValidator] = 0.7,\n        api_key: Optional[str] = None,\n    )\n</code></pre>"},{"location":"plugins/openai/#parameters","title":"Parameters","text":"<ul> <li><code>model_name</code>: The specific OpenAI model to use</li> <li><code>temperature</code>: Controls response randomness (0.0 to 1.0)</li> <li><code>api_key</code>: Optional API key for authentication</li> </ul>"},{"location":"plugins/openai/#methods","title":"Methods","text":""},{"location":"plugins/openai/#generateprompt-str-aimessage","title":"generate(prompt: str) -&gt; AIMessage","text":"<p>Generates a completion using the OpenAI API:</p> <ul> <li>Formats messages according to OpenAI's chat completion format</li> <li>Includes system message for consistent behavior</li> <li>Returns structured response in <code>AIMessage</code> format</li> <li>Includes metadata about the generation settings</li> </ul>"},{"location":"plugins/openai/#error-handling","title":"Error Handling","text":"<p>API errors are caught and wrapped in <code>BaseLLMException</code> for consistent error handling across the library.</p>"},{"location":"plugins/openai/#example-usage","title":"Example Usage","text":"<pre><code>from god_llm.plugins import ChatOpenAI\n\n# Initialize the client\nopenai = ChatOpenAI(\n    model_name=\"gpt-4\",\n    temperature=0.7\n)\n\n# Generate a response\ntry:\n    response = openai.generate(\"Explain neural networks\")\n    print(response.content)\nexcept BaseLLMException as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"plugins/overview/","title":"Plugins Overview","text":"<p>The God LLM library implements a plugin-based architecture that allows for easy integration of different LLM providers and extensibility of functionality. This document outlines the core concepts, components, and implementation details of the plugin system.</p>"},{"location":"plugins/overview/#architecture-overview","title":"Architecture Overview","text":"<p>The plugin system is built around several key components:</p>"},{"location":"plugins/overview/#1-base-classes","title":"1. Base Classes","text":""},{"location":"plugins/overview/#basellm","title":"BaseLLM","text":"<p>The foundation of the plugin system, providing the interface that all LLM providers must implement:</p> <pre><code>class BaseLLM(ABC):\n    def __init__(self, provider: LLMProviderType, api_key: Optional[str] = None):\n        self.provider = provider\n        self.auth = Auth()\n        if api_key:\n            self.auth.set_key(provider, api_key)\n        elif not self.auth.get_key(provider):\n            raise ValueError(f\"No API key provided or found for {provider}\")\n\n    @abstractmethod\n    def generate(self, prompt: str) -&gt; AIMessage:\n        pass\n</code></pre>"},{"location":"plugins/overview/#basemessage-and-derivatives","title":"BaseMessage and Derivatives","text":"<p>Message classes that standardize communication:</p> <pre><code>class BaseMessage(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n    content: str\n    created_at: datetime.datetime\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"plugins/overview/#2-provider-plugins","title":"2. Provider Plugins","text":"<p>Each LLM provider is implemented as a plugin that inherits from <code>BaseLLM</code>:</p> <ul> <li>ChatAnthropic: Anthropic Claude models</li> <li>ChatGroq: Groq LLM services</li> <li>ChatOpenAI: OpenAI GPT models</li> </ul>"},{"location":"plugins/overview/#3-type-system","title":"3. Type System","text":"<p>The library uses a robust type system to ensure consistency:</p> <pre><code>LLMProviderType = Literal[\"ollama\", \"openai\", \"anthropic\", \"bedrock\", \"groq\"]\n\nGroqModel = Literal[\n    \"gemma2-9b-it\",\n    \"llama-3.1-70b-versatile\",\n    \"llama-3.1-8b-instant\",\n    \"llama-3.2-1b-preview\",\n]\n\n# Similar literals for OpenAI and Anthropic models\n</code></pre>"},{"location":"plugins/overview/#plugin-implementation","title":"Plugin Implementation","text":""},{"location":"plugins/overview/#provider-plugin-structure","title":"Provider Plugin Structure","text":"<p>Each provider plugin follows this general structure:</p> <ol> <li>Initialization:</li> <li>Set up authentication</li> <li>Configure provider-specific client</li> <li> <p>Initialize model parameters</p> </li> <li> <p>Message Generation:</p> </li> <li>Format prompts according to provider requirements</li> <li>Handle API communication</li> <li>Process and standardize responses</li> </ol> <p>Example: <pre><code>class ChatProvider(BaseLLM):\n    def __init__(\n        self,\n        model_name: ModelType,\n        temperature: Optional[TemperatureValidator] = 0.7,\n        api_key: Optional[str] = None,\n    ):\n        super().__init__(\"provider_name\", api_key)\n        self.model_name = model_name\n        self.temperature = temperature\n        self.client = self._initialize_client()\n\n    def generate(self, prompt: str) -&gt; AIMessage:\n        try:\n            response = self._generate_response(prompt)\n            return self._format_response(response)\n        except Exception as e:\n            raise BaseLLMException(str(e))\n</code></pre></p>"},{"location":"plugins/overview/#common-features-across-plugins","title":"Common Features Across Plugins","text":"<p>All plugins implement:</p> <ol> <li>Temperature Control:</li> <li>Standardized temperature validation</li> <li> <p>Consistent application across providers</p> </li> <li> <p>Error Handling:</p> </li> <li>Provider-specific errors wrapped in <code>BaseLLMException</code></li> <li> <p>Consistent error reporting format</p> </li> <li> <p>Message Formatting:</p> </li> <li>Standardized input/output message format</li> <li> <p>Provider-specific message adaptation</p> </li> <li> <p>Metadata Handling:</p> </li> <li>Common metadata fields across providers</li> <li>Provider-specific metadata extensions</li> </ol>"},{"location":"plugins/overview/#adding-new-plugins","title":"Adding New Plugins","text":"<p>To add a new provider plugin:</p> <ol> <li>Create a new class inheriting from <code>BaseLLM</code></li> <li>Implement required methods:</li> <li><code>__init__</code></li> <li><code>generate</code></li> <li>Add provider type to <code>LLMProviderType</code></li> <li>Define model types if needed</li> <li>Implement error handling</li> </ol> <p>Example template: <pre><code>from god_llm.plugins.base import BaseLLM, AIMessage\nfrom god_llm.plugins.exceptions import BaseLLMException\n\nclass NewProvider(BaseLLM):\n    def __init__(\n        self,\n        model_name: str,\n        temperature: Optional[TemperatureValidator] = 0.7,\n        api_key: Optional[str] = None,\n    ):\n        super().__init__(\"new_provider\", api_key)\n        self.model_name = model_name\n        self.temperature = temperature\n        # Initialize provider-specific client\n\n    def generate(self, prompt: str) -&gt; AIMessage:\n        try:\n            # Implementation details here\n            return AIMessage(...)\n        except Exception as e:\n            raise BaseLLMException(f\"Error: {e}\")\n</code></pre></p>"},{"location":"plugins/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling:</li> <li>Always wrap provider-specific errors</li> <li>Provide meaningful error messages</li> <li> <p>Use appropriate error types</p> </li> <li> <p>Type Safety:</p> </li> <li>Use type hints consistently</li> <li>Define provider-specific types</li> <li> <p>Validate inputs appropriately</p> </li> <li> <p>Configuration:</p> </li> <li>Use environment variables when possible</li> <li>Provide clear configuration options</li> <li> <p>Document required settings</p> </li> <li> <p>Testing:</p> </li> <li>Write unit tests for each plugin</li> <li>Test error conditions</li> <li>Mock API responses</li> </ol>"},{"location":"plugins/overview/#future-extensions","title":"Future Extensions","text":"<p>The plugin system is designed to be extensible for future additions:</p> <ol> <li>New Providers:</li> <li>Additional LLM providers</li> <li>Different model types</li> <li> <p>Custom implementations</p> </li> <li> <p>Enhanced Features:</p> </li> <li>Streaming responses</li> <li>Batch processing</li> <li> <p>Advanced parameter control</p> </li> <li> <p>Tool Integration:</p> </li> <li>Function calling</li> <li>External tool integration</li> <li>Custom capabilities</li> </ol>"},{"location":"utils/base-format/","title":"Base Format","text":"<p>Write content for Base Format here.</p>"},{"location":"utils/base-format/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"utils/base-format/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"utils/overview/","title":"Utils Overview","text":"<p>Write content for Utils Overview here.</p>"},{"location":"utils/overview/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"utils/overview/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"},{"location":"utils/tfidf/","title":"TFIDF","text":"<p>Write content for TFIDF here.</p>"},{"location":"utils/tfidf/#overview","title":"Overview","text":"<p>TODO: Add overview content</p>"},{"location":"utils/tfidf/#further-reading","title":"Further Reading","text":"<ul> <li>Link 1</li> <li>Link 2</li> </ul>"}]}